{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark==3.4.1 spark-nlp==5.3.2 requests beautifulsoup4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpOYMAePQb74",
        "outputId": "b9fa049b-3f73-4009-e31a-6564af20f9e5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark==3.4.1\n",
            "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting spark-nlp==5.3.2\n",
            "  Downloading spark_nlp-5.3.2-py2.py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Collecting py4j==0.10.9.7 (from pyspark==3.4.1)\n",
            "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.11.12)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.15.0)\n",
            "Downloading spark_nlp-5.3.2-py2.py3-none-any.whl (564 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m565.0/565.0 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285391 sha256=718426aef95eda76d61a305e038737c4223d013ceb657c89c63117d1c3275f11\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/95/1d/739a17bda5d6a1c3c6f60eed9a82f600ab0d9fcd4c601ce0da\n",
            "Successfully built pyspark\n",
            "Installing collected packages: spark-nlp, py4j, pyspark\n",
            "  Attempting uninstall: py4j\n",
            "    Found existing installation: py4j 0.10.9.9\n",
            "    Uninstalling py4j-0.10.9.9:\n",
            "      Successfully uninstalled py4j-0.10.9.9\n",
            "  Attempting uninstall: pyspark\n",
            "    Found existing installation: pyspark 4.0.1\n",
            "    Uninstalling pyspark-4.0.1:\n",
            "      Successfully uninstalled pyspark-4.0.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dataproc-spark-connect 1.0.1 requires pyspark[connect]~=4.0.0, but you have pyspark 3.4.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed py4j-0.10.9.7 pyspark-3.4.1 spark-nlp-5.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import concat_ws, col\n",
        "from pyspark.ml.clustering import LDA\n",
        "from pyspark.ml import Pipeline\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sparknlp.base import *\n",
        "from sparknlp.annotator import *\n",
        "from pyspark.ml.feature import CountVectorizer\n",
        "import os\n",
        "import sparknlp\n",
        "from pyspark.sql import SparkSession\n",
        "import urllib.request\n",
        "from pyspark.ml.feature import IDF\n"
      ],
      "metadata": {
        "id": "84yPu9D-vqiL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalo Java 8\n",
        "\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "\n",
        "# Inicio sesion de Spark\n",
        "\n",
        "spark = sparknlp.start()\n",
        "print(f\"Versión Spark: {spark.version}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fehd3Nc8Qn1t",
        "outputId": "5de0df6b-c98b-4325-d363-8c8c49027afa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Versión Spark: 3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargo el archivo desde un repositorio de github para no tener que cargarlo manualmente cada vez que abro Colab\n",
        "\n",
        "url_archivo = \"https://github.com/juandimeglio25/TP_Prog/raw/refs/heads/main/bbc-news-data.csv\"\n",
        "archivo_local = \"bbc_news.csv\"\n",
        "\n",
        "if not os.path.exists(archivo_local):\n",
        "    print(\">>> Descargando dataset...\")\n",
        "    urllib.request.urlretrieve(url_archivo, archivo_local)\n",
        "else:\n",
        "    print(\">>> Archivo local detectado.\")\n",
        "\n",
        "df_raw = spark.read \\\n",
        "    .option(\"header\", True) \\\n",
        "    .option(\"delimiter\", \";\") \\\n",
        "    .option(\"inferSchema\", True) \\\n",
        "    .option(\"multiLine\", True) \\\n",
        "    .option(\"quote\", '\"') \\\n",
        "    .option(\"escape\", '\"') \\\n",
        "    .csv(archivo_local)\n",
        "\n",
        "df = df_raw.filter(col(\"content\").isNotNull())\n",
        "\n",
        "df = df.withColumn(\"texto_completo\", concat_ws(\" \", col(\"title\"), col(\"content\")))\n",
        "\n",
        "print(\"Esquema final y muestra:\")\n",
        "df.printSchema()\n",
        "df.select(\"title\", \"texto_completo\").show(2, truncate=80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asetInA8T-a0",
        "outputId": "0a254c4f-99fc-494a-b064-e25727c6d38a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Descargando dataset...\n",
            "Esquema final y muestra:\n",
            "root\n",
            " |-- category: string (nullable = true)\n",
            " |-- filename: string (nullable = true)\n",
            " |-- title: string (nullable = true)\n",
            " |-- content: string (nullable = true)\n",
            " |-- texto_completo: string (nullable = false)\n",
            "\n",
            "+---------------------------------+--------------------------------------------------------------------------------+\n",
            "|                            title|                                                                  texto_completo|\n",
            "+---------------------------------+--------------------------------------------------------------------------------+\n",
            "|Ad sales boost Time Warner profit|Ad sales boost Time Warner profit  Quarterly profits at US media giant TimeWa...|\n",
            "| Dollar gains on Greenspan speech|Dollar gains on Greenspan speech  The dollar has hit its highest level agains...|\n",
            "+---------------------------------+--------------------------------------------------------------------------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EMpiezo con el pipeline de procesamiento: Convierto texto a formato documento\n",
        "document_assembler = DocumentAssembler() \\\n",
        "    .setInputCol(\"texto_completo\") \\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "# Tokenizo\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "# Normalizo\n",
        "normalizer = Normalizer() \\\n",
        "    .setInputCols([\"token\"]) \\\n",
        "    .setOutputCol(\"normalized\") \\\n",
        "    .setLowercase(True)\n",
        "\n",
        "# Elimino Stopwords (palabras vacías) y adicionales para que no manchen el modelo, como pueden ser algunos verbos o pronombres\n",
        "palabras_basura = [\"mr\", \"said\", \"would\", \"year\", \"also\", \"new\", \"one\",\"our\", \"ours\", \"ourselves\", \"yours\", \"my\", \"me\",\"going\", \"gone\", \"went\", \"goes\", \"two\",\"ba\",\"bn\", \"last\", \"first\",\"we\", \"m\", \"make\", \"play\", \"win\", \"get\", \"use\", \"go\", \"take\", \"say\", \"tell\", \"come\", \"see\", \"us\"]\n",
        "\n",
        "stopwords_cleaner = StopWordsCleaner.pretrained(\"stopwords_en\", \"en\") \\\n",
        "    .setInputCols([\"normalized\"]) \\\n",
        "    .setOutputCol(\"clean_tokens\") \\\n",
        "    .setCaseSensitive(False) \\\n",
        "    .setStopWords(StopWordsCleaner.loadDefaultStopWords(\"english\") + palabras_basura)\n",
        "\n",
        "# Lematizo\n",
        "lemmatizer = LemmatizerModel.pretrained(\"lemma_antbnc\", \"en\") \\\n",
        "    .setInputCols([\"clean_tokens\"]) \\\n",
        "    .setOutputCol(\"lemma\")\n",
        "\n",
        "# Finisher (agarro los resultados)\n",
        "finisher = Finisher() \\\n",
        "    .setInputCols([\"lemma\"]) \\\n",
        "    .setOutputCols([\"tokens_finales\"]) \\\n",
        "    .setCleanAnnotations(False)\n",
        "\n",
        "# Con esto ejecuto el pipeline\n",
        "nlp_pipeline = Pipeline(stages=[\n",
        "    document_assembler,\n",
        "    tokenizer,\n",
        "    normalizer,\n",
        "    stopwords_cleaner,\n",
        "    lemmatizer,\n",
        "    finisher\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkPqi2Wa6JTI",
        "outputId": "672d373d-8be1-4f12-f878-590e46d7003f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stopwords_en download started this may take some time.\n",
            "Approximate size to download 2.9 KB\n",
            "[OK!]\n",
            "lemma_antbnc download started this may take some time.\n",
            "Approximate size to download 907.6 KB\n",
            "[OK!]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fit() entrena el pipeline\n",
        "model_nlp = nlp_pipeline.fit(df)\n",
        "\n",
        "# transform() aplica la lógica a los datos\n",
        "processed_df = model_nlp.transform(df)\n",
        "\n",
        "# Verifico que la lematización funcionó (usando la corrección de .result)\n",
        "processed_df.select(\n",
        "    col(\"title\"),\n",
        "    col(\"lemma.result\").alias(\"ejemplo_lemas\")\n",
        ").show(5, truncate=80)"
      ],
      "metadata": {
        "id": "lRN9jm5U6LlL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e64fd384-821a-4a28-edb4-2cc8fbbe7052"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------+--------------------------------------------------------------------------------+\n",
            "|                            title|                                                                   ejemplo_lemas|\n",
            "+---------------------------------+--------------------------------------------------------------------------------+\n",
            "|Ad sales boost Time Warner profit|[ad, sales, boost, time, warner, profit, quarterly, profit, media, giant, tim...|\n",
            "| Dollar gains on Greenspan speech|[dollar, gain, greenspan, speech, dollar, hit, high, level, euro, almost, thr...|\n",
            "|Yukos unit buyer faces loan claim|[yukos, unit, buyer, face, loan, claim, owner, embattled, russian, oil, giant...|\n",
            "|High fuel prices hit BA's profits|[high, fuel, price, hit, ba, profit, british, airway, blame, high, fuel, pric...|\n",
            "|Pernod takeover talk lifts Domecq|[pernod, takeover, talk, lift, domecq, share, uk, drink, food, firm, ally, do...|\n",
            "+---------------------------------+--------------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculo la TF\n",
        "cv = CountVectorizer(\n",
        "    inputCol=\"tokens_finales\",\n",
        "    outputCol=\"features\",\n",
        "    vocabSize=1000,\n",
        "    minDF=5.0\n",
        ")\n",
        "\n",
        "cv_model = cv.fit(processed_df)\n",
        "df_vectorizado = cv_model.transform(processed_df)\n",
        "\n",
        "# Cacheo para optimizar\n",
        "df_vectorizado.cache()\n",
        "\n",
        "# Agrego una 'seed' para que los resultados no cambien cada vez que se ejecuta\n",
        "lda = LDA(k=5, maxIter=10, seed=1234)\n",
        "model_lda = lda.fit(df_vectorizado)\n"
      ],
      "metadata": {
        "id": "ORPKNGkB6NjU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = cv_model.vocabulary\n",
        "\n",
        "# Indices de los términos más importantes por tema\n",
        "topics = model_lda.describeTopics(maxTermsPerTopic=10)\n",
        "topics_rdd = topics.rdd\n",
        "\n",
        "# Función auxiliar para mapear índices numéricos a palabras reales\n",
        "def map_indices_to_words(row):\n",
        "    topic_indices = row['termIndices']\n",
        "    terms = [vocab[idx] for idx in topic_indices]\n",
        "    return (row['topic'], terms)\n",
        "\n",
        "print(\"RESULTADOS DEL ANÁLISIS DE TÓPICOS:\")\n",
        "results = topics_rdd.map(map_indices_to_words).collect()\n",
        "\n",
        "for topic, words in results:\n",
        "    print(f\"Tema {topic}: {', '.join(words)}\")"
      ],
      "metadata": {
        "id": "Oh2YEI7m6P8o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3717d769-9081-415f-9dd8-e077fe7ab533"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RESULTADOS DEL ANÁLISIS DE TÓPICOS:\n",
            "Tema 0: bank, rise, market, growth, rate, fall, party, economy, figure, december\n",
            "Tema 1: game, show, time, good, take, well, make, people, second, world\n",
            "Tema 2: people, firm, use, company, phone, music, mobile, technology, user, service\n",
            "Tema 3: film, good, award, star, win, england, year, world, include, club\n",
            "Tema 4: government, minister, plan, country, company, tell, firm, uk, report, deal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF\n",
        "idf = IDF(inputCol=\"features\", outputCol=\"tfidf_features\")\n",
        "\n",
        "idf_model = idf.fit(df_vectorizado)\n",
        "\n",
        "df_tfidf = idf_model.transform(df_vectorizado)\n",
        "print(\">>> Vectores TF-IDF generados:\")\n",
        "df_tfidf.select(\"title\", \"tfidf_features\").show(5, truncate=50)"
      ],
      "metadata": {
        "id": "gygPvInxD-NA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29320502-7b31-4a30-c5be-71e1f370a2ec"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Vectores TF-IDF generados:\n",
            "+---------------------------------+--------------------------------------------------+\n",
            "|                            title|                                    tfidf_features|\n",
            "+---------------------------------+--------------------------------------------------+\n",
            "|Ad sales boost Time Warner profit|(1000,[2,9,10,11,15,21,27,29,30,31,35,37,39,41,...|\n",
            "| Dollar gains on Greenspan speech|(1000,[2,4,7,10,13,15,19,21,29,33,35,37,39,42,4...|\n",
            "|Yukos unit buyer faces loan claim|(1000,[11,14,16,21,28,71,72,78,97,98,113,117,12...|\n",
            "|High fuel prices hit BA's profits|(1000,[1,4,6,8,10,13,14,25,29,30,35,41,42,44,45...|\n",
            "|Pernod takeover talk lifts Domecq|(1000,[2,10,13,15,18,28,35,41,45,46,49,60,72,73...|\n",
            "+---------------------------------+--------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Entreno LDA usando los vectores TF-IDF en lugar de los conteos simples\n",
        "lda_tfidf = LDA(k=5, maxIter=10, featuresCol=\"tfidf_features\")\n",
        "model_lda_tfidf = lda_tfidf.fit(df_tfidf)\n"
      ],
      "metadata": {
        "id": "WlizclElHZ_C"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Comparo conteo simple con TF-IDF\n",
        "\n",
        "print(\">>> Evaluando Modelo 1 (Conteo Simple):\")\n",
        "ll_counts = model_lda.logLikelihood(df_vectorizado)\n",
        "lp_counts = model_lda.logPerplexity(df_vectorizado)\n",
        "print(f\"Log Likelihood: {ll_counts}\")\n",
        "print(f\"Log Perplexity: {lp_counts}\")\n",
        "\n",
        "print(\"-\" * 30)\n",
        "\n",
        "\n",
        "# Entreno un nuevo modelo LDA específico para TF-IDF\n",
        "lda_tfidf = LDA(k=5, maxIter=10, featuresCol=\"tfidf_features\")\n",
        "model_lda_tfidf = lda_tfidf.fit(df_tfidf)\n",
        "\n",
        "print(\">>> Evaluando Modelo 2 (TF-IDF):\")\n",
        "ll_tfidf = model_lda_tfidf.logLikelihood(df_tfidf)\n",
        "lp_tfidf = model_lda_tfidf.logPerplexity(df_tfidf)\n",
        "\n",
        "print(f\"Log Likelihood (TF-IDF): {ll_tfidf}\")\n",
        "print(f\"Log Perplexity (TF-IDF): {lp_tfidf}\")"
      ],
      "metadata": {
        "id": "GcH04YJiJ4cB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bdfac30-54ea-4c63-9385-74dfabeb4ff8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Evaluando Modelo 1 (Conteo Simple):\n",
            "Log Likelihood: -1784641.015852126\n",
            "Log Perplexity: 6.525338822248846\n",
            "------------------------------\n",
            ">>> Evaluando Modelo 2 (TF-IDF):\n",
            "Log Likelihood (TF-IDF): -4218207.366975071\n",
            "Log Perplexity (TF-IDF): 6.6120093212875295\n"
          ]
        }
      ]
    }
  ]
}